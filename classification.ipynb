{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
    "from sklearn.metrics import confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load training and testing data\n",
    "df_train = pd.read_csv('training.csv')\n",
    "df_test = pd.read_csv('testing.csv')\n",
    "\n",
    "# drop meaningless first column\n",
    "df_train.drop(columns='Unnamed: 0', inplace=True)\n",
    "df_test.drop(columns='Unnamed: 0', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# preprocess X, y\n",
    "def getInput(df, target):\n",
    "    selected_features = (\n",
    "        df['actual_weight'].values,\n",
    "        df['declared_horse_weight'].values,\n",
    "        df['draw'].values,\n",
    "        df['recent_ave_rank'].values,\n",
    "        df['jockey_ave_rank'].values,\n",
    "        df['trainer_ave_rank'].values,\n",
    "        df['race_distance'].values\n",
    "    )\n",
    "    scaler = preprocessing.StandardScaler()\n",
    "    X_standardized = scaler.fit_transform(selected_features)\n",
    "    X = np.stack(X_standardized, axis=1)\n",
    "    y = df[target]\n",
    "    return X, y\n",
    "\n",
    "# print performance\n",
    "def printPerformance(y_true, y_predict):\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_predict).ravel()\n",
    "    print('TN: {} | FP: {}'.format(tn, fp))\n",
    "    print('FN: {} | TP: {}'.format(fn, tp))\n",
    "    print(classification_report(y_true, y_predict))\n",
    "\n",
    "# calculate ground truth\n",
    "def getTrueLabel(df):\n",
    "    horseWin = []\n",
    "    horseRankTop3 = []\n",
    "    horseRankTop50Percent = []\n",
    "    start = 0\n",
    "    while start != len(df):\n",
    "        end = start\n",
    "        while end < len(df) and df['race_id'][start] == df['race_id'][end]:\n",
    "            end += 1\n",
    "        for i in range(start, end):\n",
    "            horse = df.iloc[i]\n",
    "            if horse['finishing_position'] == 1:\n",
    "                horseWin.append(1)\n",
    "            else:\n",
    "                horseWin.append(0)\n",
    "            if horse['finishing_position'] <= 3:\n",
    "                horseRankTop3.append(1)\n",
    "            else:\n",
    "                horseRankTop3.append(0)\n",
    "            if horse['finishing_position'] < (end-start)/2:\n",
    "                horseRankTop50Percent.append(1)\n",
    "            else:\n",
    "                horseRankTop50Percent.append(0)\n",
    "        start = end\n",
    "    return horseWin, horseRankTop3, horseRankTop50Percent\n",
    "\n",
    "horseWin, horseRankTop3, horseRankTop50Percent = getTrueLabel(df_train)\n",
    "df_train['HorseWin'] = horseWin\n",
    "df_train['HorseRankTop3'] = horseRankTop3\n",
    "df_train['HorseRankTop50Percent'] = horseRankTop50Percent\n",
    "horseWin, horseRankTop3, horseRankTop50Percent = getTrueLabel(df_test)\n",
    "df_test['HorseWin'] = horseWin\n",
    "df_test['HorseRankTop3'] = horseRankTop3\n",
    "df_test['HorseRankTop50Percent'] = horseRankTop50Percent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TN: 13783 | FP: 7834\n",
      "FN: 731 | TP: 1152\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.95      0.64      0.76     21617\n",
      "          1       0.13      0.61      0.21      1883\n",
      "\n",
      "avg / total       0.88      0.64      0.72     23500\n",
      "\n",
      "TN: 3247 | FP: 2139\n",
      "FN: 136 | TP: 342\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.96      0.60      0.74      5386\n",
      "          1       0.14      0.72      0.23       478\n",
      "\n",
      "avg / total       0.89      0.61      0.70      5864\n",
      "\n",
      "TN: 11082 | FP: 6783\n",
      "FN: 2184 | TP: 3451\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.84      0.62      0.71     17865\n",
      "          1       0.34      0.61      0.43      5635\n",
      "\n",
      "avg / total       0.72      0.62      0.65     23500\n",
      "\n",
      "TN: 2696 | FP: 1738\n",
      "FN: 363 | TP: 1067\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.88      0.61      0.72      4434\n",
      "          1       0.38      0.75      0.50      1430\n",
      "\n",
      "avg / total       0.76      0.64      0.67      5864\n",
      "\n",
      "TN: 8269 | FP: 5266\n",
      "FN: 3801 | TP: 6164\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.69      0.61      0.65     13535\n",
      "          1       0.54      0.62      0.58      9965\n",
      "\n",
      "avg / total       0.62      0.61      0.62     23500\n",
      "\n",
      "TN: 2116 | FP: 1272\n",
      "FN: 655 | TP: 1821\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.76      0.62      0.69      3388\n",
      "          1       0.59      0.74      0.65      2476\n",
      "\n",
      "avg / total       0.69      0.67      0.67      5864\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# logistic regression CV\n",
    "lr_model = LogisticRegressionCV(cv=10, class_weight='balanced', refit=False)\n",
    "# HorseWin\n",
    "X_train, y_train = getInput(df_train, 'HorseWin')\n",
    "lr_model.fit(X_train, y_train)\n",
    "y_predict_train = lr_model.predict(X_train)\n",
    "X_test, y_test = getInput(df_test, 'HorseWin')\n",
    "y_predict_test = lr_model.predict(X_test)\n",
    "printPerformance(y_train, y_predict_train)\n",
    "printPerformance(y_test, y_predict_test)\n",
    "# HorseRankTop3\n",
    "X_train, y_train = getInput(df_train, 'HorseRankTop3')\n",
    "lr_model.fit(X_train, y_train)\n",
    "y_predict_train = lr_model.predict(X_train)\n",
    "X_test, y_test = getInput(df_test, 'HorseRankTop3')\n",
    "y_predict_test = lr_model.predict(X_test)\n",
    "printPerformance(y_train, y_predict_train)\n",
    "printPerformance(y_test, y_predict_test)\n",
    "# HorseRankTop50Percent\n",
    "X_train, y_train = getInput(df_train, 'HorseRankTop50Percent')\n",
    "lr_model.fit(X_train, y_train)\n",
    "y_predict_train = lr_model.predict(X_train)\n",
    "X_test, y_test = getInput(df_test, 'HorseRankTop50Percent')\n",
    "y_predict_test = lr_model.predict(X_test)\n",
    "printPerformance(y_train, y_predict_train)\n",
    "printPerformance(y_test, y_predict_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# NaÃ¯ve Bayes\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
